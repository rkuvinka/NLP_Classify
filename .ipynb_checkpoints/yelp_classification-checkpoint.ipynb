{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Make Training and Dev Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def csv_to_array(filename):\n",
    "    file = open(filename, 'r')\n",
    "    file_text = file.read()\n",
    "    # Get rid of the first three, which are examples\n",
    "    split_text = file_text.split('|')[3:]\n",
    "    i = 0\n",
    "    tuple_text = []\n",
    "    while i < len(split_text) - 1:\n",
    "        tuple_text.append((split_text[i+1], split_text[i], int(i/2)))\n",
    "        i += 2\n",
    "    \n",
    "    return tuple_text\n",
    "    \n",
    "def make_training_and_dev_data(all_data):\n",
    "    random.shuffle(all_data)\n",
    "    training_ratio = .999\n",
    "    return all_data[:int(len(all_data) * training_ratio)], all_data[int(len(all_data) * training_ratio):]\n",
    "#     return all_data[:(len(all_data) * training_ratio)], all_data[:(len(all_data) * training_ratio)] \n",
    "\n",
    "\n",
    "official_data = csv_to_array('yelp_data_official_training.csv')\n",
    "\n",
    "for garbage in official_data:\n",
    "    if garbage[1].isdigit() is False:\n",
    "        print(garbage[1])\n",
    "        break\n",
    "\n",
    "training_data, dev_data = make_training_and_dev_data(official_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reviews(data):\n",
    "    reviews = []\n",
    "    for i in range(len(data)):\n",
    "        reviews.append(data[i][0])\n",
    "    return reviews\n",
    "\n",
    "def labels(data):\n",
    "    reviews = []\n",
    "    for i in range(len(data)):\n",
    "        reviews.append(int(data[i][1]))\n",
    "    return reviews\n",
    "\n",
    "reviews_training = reviews(training_data)\n",
    "label_training = labels(training_data)\n",
    "reviews_dev = reviews(dev_data)\n",
    "label_dev = labels(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47952, 95608)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(reviews_training)\n",
    "X_train_counts.shape\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Naive Bayes Pipeline</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77551020408163263"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf_mnb = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),])\n",
    "\n",
    "text_clf_mnb = text_clf_mnb.fit(reviews_training, label_training)\n",
    "predicted_mnb = text_clf_mnb.predict(reviews_dev)\n",
    "np.mean(predicted_mnb == label_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>SVM Pipeline</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "svm_classifier = SGDClassifier(loss='hinge',\n",
    "                               penalty='l2',\n",
    "                               alpha=1e-3,\n",
    "                               n_iter=10,\n",
    "                               random_state=42)\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(ngram_range=(1, 3), max_df=0.95, min_df=2,stop_words='english')),\n",
    "                     #('tfidf', TfidfTransformer()), \n",
    "                     ('clf', svm_classifier),])\n",
    "text_clf_svm = text_clf_svm.fit(reviews_training, label_training)\n",
    "predicted_svm = text_clf_svm.predict(reviews_dev)\n",
    "np.mean(predicted_svm == label_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load Test Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def test_file_to_array(filename):\n",
    "    file = open(filename, 'r')\n",
    "    file_text = file.read()\n",
    "    # Skip the first one because it's the header starter shit\n",
    "    split_text = re.split('\\d+\\|', file_text)[1:]\n",
    "    tupled_text = []\n",
    "    for i in range(len(split_text)):\n",
    "        tupled_text.append((split_text[i], i))        \n",
    "    return tupled_text\n",
    "\n",
    "test_array = test_file_to_array('yelp_data_official_test_nocategories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Make Test Predictions For All Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicted_svm = text_clf_svm.predict(test_reviews)\n",
    "# predicted_mnb = text_clf_mnb.predict(test_reviews)\n",
    "\n",
    "def test_prediction(model):\n",
    "    output = []\n",
    "    test_reviews = []\n",
    "    test_id = []\n",
    "    \n",
    "    for i in test_array:\n",
    "        test_reviews.append(i[0])\n",
    "        test_id.append(i[1])\n",
    "        \n",
    "    if str(model) == 'svm':\n",
    "        predicted = text_clf_svm.predict(test_reviews)\n",
    "        \n",
    "    elif model == 'mnb':\n",
    "        predicted = text_clf_mnb.predict(test_reviews)\n",
    "        \n",
    "    for i in range(0,len(predicted)):\n",
    "        output.append((int(i),predicted[i]))\n",
    "    \n",
    "    return output\n",
    "\n",
    "output_mnb = test_prediction('mnb')\n",
    "output_svm = test_prediction('svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Write Output to CSV: Specify Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_output_to_csv(output, filename):\n",
    "    file = open(filename, 'w')\n",
    "    file.write('Id' + ',' + 'Category' + '\\n' )\n",
    "    if str(output) == str(output_mnb):\n",
    "        for item in output_mnb:\n",
    "            file.write(str(item[0]) + ',' + str(item[1]) + '\\n')\n",
    "    elif str(output) == str(output_svm):\n",
    "        for item in output_svm:\n",
    "            file.write(str(item[0]) + ',' + str(item[1]) + '\\n')\n",
    "\n",
    "write_output_to_csv(output_svm, 'kegel_boys_5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classification Report and Confustion Matrix</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------SVM-------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00        16\n",
      "          2       0.80      0.80      0.80         5\n",
      "          3       1.00      0.94      0.97        16\n",
      "          4       0.80      0.89      0.84         9\n",
      "          6       1.00      1.00      1.00         3\n",
      "\n",
      "avg / total       0.94      0.94      0.94        49\n",
      "\n",
      "\n",
      "--------MNB-------\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.62      1.00      0.76        16\n",
      "          2       0.75      0.60      0.67         5\n",
      "          3       1.00      0.88      0.93        16\n",
      "          4       1.00      0.56      0.71         9\n",
      "          6       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.79      0.78      0.75        49\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabe/anaconda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('--------SVM-------' + '\\n')\n",
    "print(metrics.classification_report(label_dev, predicted_svm)+ '\\n')\n",
    "print('--------MNB-------' + '\\n')\n",
    "print(metrics.classification_report(label_dev, predicted_mnb)+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------SVM-------\n",
      "\n",
      "[[16  0  0  0  0]\n",
      " [ 0  4  0  1  0]\n",
      " [ 0  0 15  1  0]\n",
      " [ 0  1  0  8  0]\n",
      " [ 0  0  0  0  3]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('--------SVM-------' + '\\n')\n",
    "print(metrics.confusion_matrix(label_dev, predicted_svm),'\\n')\n",
    "# print('--------MNB-------' + '\\n')\n",
    "# print(metrics.confusion_matrix(label_dev, predicted_mnb),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dot Product of Result Sets (Not sure if useful)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 5, 3, 3]\n",
      "[4, 4, 3, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 6, 5, 5, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "67057"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = [int(i[1]) for i in output_mnb]\n",
    "svm = [int(i[1]) for i in output_svm]\n",
    "print(mnb[190:210])\n",
    "print(svm[190:210])\n",
    "np.inner(svm,mnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h3>Let's look at our mistakes</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This dry cleaner may be the least professional, and poorly managed of any that I have ever been to. I have had atleast 3 bad experiences including poor/no pressing on dry cleaned items, items being returned without being cleaned, however, this last experience put a nail in the coffin. I took one of my favorite work dresses here to be routinely dry cleaned (they had cleaned it 3 previous times without a major issue), and it was returned to me COMPLETELY ruined. It had black ink ran all over the dress, even so much that it was on the tag they attached to it. I returned it to them, told that \"\"it happens sometimes\"\", and then told I would be compensated. It has been over a week, and still no call from the manager with a resolution, or time frame for reimbursement. Its clear that in this circumstance you get what you pay for!\"\n",
      "17726 4 2\n",
      "\"Everyone in my family has been going here for years and years, squeezing the last miles out of beloved shoes and boots. Every neighborhood ought to have one of these shops. Sally does a great job, in this day and age of shoddy workmanship, to get shoes back to the way they were.\n",
      "Support this shop! We all need places like this to make life saner.\"\n",
      "46234 2 4\n",
      "\"This management team is stellar.  From reception to management there is always a friendly person available during business hours.  They pick up your mail when you're out of town and deliver UPS/FedEx packages during the day when you're not there to sign for it.  Any time there is a maintenance issue someone will come by the day of or next day and fix the problem, even if I'm not available to be there.\n",
      "\n",
      "I had to move out recently due to a relocation to another city, and I wish I could find a company like Franklin West here.  They sent my security deposit refund quickly and set the bar really high for my future renting endeavors.\"\n",
      "222 4 3\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(predicted_svm)):\n",
    "    if predicted_svm[i] != label_dev[i]:\n",
    "        print(reviews_training[i],predicted_svm[i],label_dev[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
